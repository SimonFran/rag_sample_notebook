{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcde1c9",
   "metadata": {},
   "source": [
    "# RAG General Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216460f3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook shows a minimal Retrieval-Augmented Generation (RAG) workflow:\n",
    "- Download short context paragraphs from a public dataset (SQuAD validation).\n",
    "- Chunk and deduplicate the text.\n",
    "- Compute embeddings using Ollama embeddings and store them in PostgreSQL using `pgvector` (via `langchain_postgres.PGVector`).\n",
    "- Build a retriever + LLM chain to answer questions grounded in the stored contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a2dbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import os\n",
    "import psycopg\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b20cd",
   "metadata": {},
   "source": [
    "### Database Configuration And Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb093c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "CONNECTION_STRING = os.getenv(\"DATABASE_URL\")\n",
    "COLLECTION_NAME = \"general_knowledge\"\n",
    "\n",
    "if not CONNECTION_STRING:\n",
    "    raise RuntimeError(\"DATABASE_URL not set. Please set the environment variable or create a local .env file. See .env.example.\")\n",
    "\n",
    "# Used to limit the number of paragraphs processed and therefore the resource usage\n",
    "MAX_PARAGRAPH_COUNT = 150000 \n",
    "\n",
    "# Used to not overload Postgres with too many parameters\n",
    "DB_BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8eed1d",
   "metadata": {},
   "source": [
    "### 1. Database Cleanup\n",
    "To always start with a fresh database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "097db36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning database...\n",
      " -> Database cleared.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning database...\")\n",
    "try:\n",
    "    raw_conn_str = CONNECTION_STRING.replace(\"+psycopg\", \"\")\n",
    "    with psycopg.connect(raw_conn_str) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"DROP TABLE IF EXISTS langchain_pg_embedding CASCADE;\")\n",
    "            cur.execute(\"DROP TABLE IF EXISTS langchain_pg_collection CASCADE;\")\n",
    "        conn.commit()\n",
    "    print(\" -> Database cleared.\")\n",
    "except Exception as e:\n",
    "    print(f\" -> Cleanup skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d664bb",
   "metadata": {},
   "source": [
    "### 2. Ollama & POSTGRES Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07c675ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION_STRING,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "vector_store.create_tables_if_not_exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028c90f",
   "metadata": {},
   "source": [
    "### 3. Download Dataset From Huggingface\n",
    "[SQuAD dataset on Hugging Face](https://huggingface.co/datasets/rajpurkar/squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5e1dac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SQuAD dataset...\n",
      "Selecting first 150000 unique paragraphs...\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading SQuAD dataset...\")\n",
    "ds = load_dataset(\"squad\", split=\"validation\")\n",
    "\n",
    "unique_contexts = set()\n",
    "raw_docs = []\n",
    "\n",
    "print(f\"Selecting first {MAX_PARAGRAPH_COUNT} unique paragraphs...\")\n",
    "\n",
    "for row in ds:\n",
    "    text = row['context']\n",
    "    title = row['title']\n",
    "    \n",
    "    # Removing duplicates in Dataset\n",
    "    if text not in unique_contexts:\n",
    "        unique_contexts.add(text)\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=text,\n",
    "            metadata={\"title\": title, \"source\": \"wikipedia\"}\n",
    "        )\n",
    "        raw_docs.append(doc)\n",
    "    \n",
    "    if len(raw_docs) >= MAX_PARAGRAPH_COUNT:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ebf0fc",
   "metadata": {},
   "source": [
    "### 4. Chunk Data\n",
    "\n",
    "This cell splits the collected raw documents into fixed-size, overlapping chunks that are ready for embedding and insertion into the vector store.\n",
    "\n",
    "Purpose: produce manageable-length passages for embedding, while the overlap preserves context across chunk boundaries so retrieved chunks remain coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e5409aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 2067 raw documents...\n",
      " -> Generated 2472 chunks to embed.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Splitting {len(raw_docs)} raw documents...\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(raw_docs)\n",
    "print(f\" -> Generated {len(split_docs)} chunks to embed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86712a3",
   "metadata": {},
   "source": [
    "### 5. Ingestion\n",
    "\n",
    "We split the collected texts into smaller chunks so embeddings and retrieval are manageable. Chunking keeps passage length within model/context limits, improves relevance of retrieved passages, and helps preserve local coherence (often using overlap) across chunk boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "353e5f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and inserting...\n",
      " -> Inserted batch 0 to 1000 (Total: 1000)\n",
      " -> Inserted batch 1000 to 2000 (Total: 2000)\n",
      " -> Inserted batch 2000 to 3000 (Total: 2472)\n",
      "✅ Succesfully completed ingestion.\n"
     ]
    }
   ],
   "source": [
    "# Generate UUIDs for every single chunk\n",
    "doc_ids = [str(uuid.uuid4()) for _ in split_docs]\n",
    "\n",
    "print(\"Embedding and inserting...\")\n",
    "total_inserted = 0\n",
    "\n",
    "# Loop in batches to support larger datasets\n",
    "for i in range(0, len(split_docs), DB_BATCH_SIZE):\n",
    "    batch_end = i + DB_BATCH_SIZE\n",
    "    \n",
    "    batch_docs = split_docs[i:batch_end]\n",
    "    batch_ids = doc_ids[i:batch_end]\n",
    "    \n",
    "    try:\n",
    "        vector_store.add_documents(documents=batch_docs, ids=batch_ids)\n",
    "        total_inserted += len(batch_docs)\n",
    "        print(f\" -> Inserted batch {i} to {batch_end} (Total: {total_inserted})\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error on batch {i}: {e}\")\n",
    "\n",
    "print(\"✅ Succesfully completed ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2e580",
   "metadata": {},
   "source": [
    "### Setup Retriever And LLM\n",
    "We look for the 10 most relevant chunks. Setting temperature to a low value seems to be recommended for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9472088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:32b\",\n",
    "    temperature=0.1,    # Keep low for factual answers\n",
    "    num_ctx=16384,      # Increase context window to fit all retrieved docs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b11a81",
   "metadata": {},
   "source": [
    "### Define The Prompt\n",
    "\n",
    "In `context` the Postgres search result will be provided. `context` will hold the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec728bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an expert research assistant. \n",
    "Answer the question based ONLY on the following context.\n",
    "If the context contains conflicting information, note the conflict.\n",
    "If the answer is not in the context, state that you do not know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764051d",
   "metadata": {},
   "source": [
    "### Build The Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6a29ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"[Source: {d.metadata.get('title', 'Unknown')}]\\n{d.page_content}\" for d in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0e1b51",
   "metadata": {},
   "source": [
    "### Run Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65d78b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying LLM: 'Why was the Apollo Space Program significant?'\n",
      "\n",
      "--- ANSWER ---\n",
      "The Apollo Space Program was significant for several reasons:\n",
      "\n",
      "- It achieved major human spaceflight milestones, including being the first to send manned missions beyond low Earth orbit.\n",
      "- The program marked the first time humans landed on another celestial body with Apollo 8 and completed six Moon landings by the end of Apollo 17.\n",
      "- It returned a substantial amount (842 pounds or 382 kg) of lunar rocks and soil, which greatly contributed to scientific understanding about the Moon's composition and geological history.\n",
      "- The program laid foundational capabilities for NASA’s future human spaceflight endeavors and funded critical infrastructure like its Johnson Space Center and Kennedy Space Center.\n",
      "- It spurred technological advancements in various fields such as avionics, telecommunications, and computers.\n",
      "\n",
      "These achievements not only advanced space exploration but also had significant impacts on technology and science.\n"
     ]
    }
   ],
   "source": [
    "question = \"Why was the Apollo Space Program significant?\"\n",
    "\n",
    "print(f\"Querying LLM: '{question}'\")\n",
    "\n",
    "print(\"\\n--- ANSWER ---\")\n",
    "\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
